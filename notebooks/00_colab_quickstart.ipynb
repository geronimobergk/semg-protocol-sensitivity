{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5-10 min)\n",
        "\n",
        "This notebook installs the package from Git and writes a minimal experiment config next to the notebook.\n",
        "\n",
        "It runs the full end-to-end pipeline on a tiny fixture dataset (no NinaPro download):\n",
        "`prepare -> splits -> traineval -> report`, then runs `size` for quick sizing estimates.\n",
        "\n",
        "Outputs are written under `runs/colab_quickstart/`.\n",
        "\n",
        "> This is a tutorial run (smoke-like config), not a benchmark for reporting results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "be37ccd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package already available - skipping install\n"
          ]
        }
      ],
      "source": [
        "def sh(cmd):\n",
        "    print(\"+\", \" \".join(map(str, cmd)))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "GIT_URL = (\n",
        "    \"git+https://github.com/geronimobergk/semg-protocol-sensitivity.git\"\n",
        "    \"@fix/colab-notebook\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    import tinyml_semg_classifier  # noqa F401\n",
        "\n",
        "    print(\"Package already available - skipping install\")\n",
        "except Exception:\n",
        "    repo_root = Path.cwd().resolve()\n",
        "    if (repo_root / \"pyproject.toml\").exists():\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_root)])\n",
        "    elif (repo_root.parent / \"pyproject.toml\").exists():\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_root.parent)])\n",
        "    else:\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", GIT_URL])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: 3.14.0 | torch: 2.9.1 | cuda: False\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a tiny experiment config\n",
        "\n",
        "We generate a small fixture and write a minimal smoke-like config next to the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config written to: /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/colab_quickstart.yaml\n",
            "Outputs root: /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd().resolve()\n",
        "OUT_ROOT = (NOTEBOOK_DIR / \"runs\" / \"colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fixture_path = OUT_ROOT / \"fixture_tiny.npz\"\n",
        "\n",
        "\n",
        "def fixture_needs_regen(path: Path) -> bool:\n",
        "    if not path.exists():\n",
        "        return True\n",
        "    try:\n",
        "        with np.load(path) as data:\n",
        "            required = [\n",
        "                \"X\",\n",
        "                \"subject_id\",\n",
        "                \"rep_id\",\n",
        "                \"gesture_id\",\n",
        "                \"exercise_id\",\n",
        "                \"sample_start\",\n",
        "                \"sample_end\",\n",
        "            ]\n",
        "            if any(name not in data for name in required):\n",
        "                return True\n",
        "            arrays = [np.asarray(data[name]) for name in required[1:]]\n",
        "            num_rows = arrays[0].shape[0]\n",
        "            if any(arr.shape[0] != num_rows for arr in arrays):\n",
        "                return True\n",
        "            rows = set(zip(*(arr.tolist() for arr in arrays)))\n",
        "            return len(rows) != num_rows\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "if fixture_needs_regen(fixture_path):\n",
        "    rng = np.random.default_rng(0)\n",
        "    subjects = [1, 2]\n",
        "    reps = [1, 2, 3]\n",
        "    gestures = [1, 2]\n",
        "    windows_per_combo = 5\n",
        "    records = [\n",
        "        (subject, rep, gesture)\n",
        "        for subject in subjects\n",
        "        for rep in reps\n",
        "        for gesture in gestures\n",
        "        for _ in range(windows_per_combo)\n",
        "    ]\n",
        "    num_windows = len(records)\n",
        "    num_electrodes = 2\n",
        "    num_samples = 8\n",
        "\n",
        "    X = rng.standard_normal((num_windows, num_electrodes, num_samples)).astype(\n",
        "        \"float32\"\n",
        "    )\n",
        "    subject_id = np.array([r[0] for r in records], dtype=\"int32\")\n",
        "    rep_id = np.array([r[1] for r in records], dtype=\"int32\")\n",
        "    gesture_id = np.array([r[2] for r in records], dtype=\"int32\")\n",
        "    exercise_id = np.ones(num_windows, dtype=\"int32\")\n",
        "    sample_start = np.arange(num_windows, dtype=\"int32\") * num_samples\n",
        "    sample_end = sample_start + (num_samples - 1)\n",
        "\n",
        "    np.savez(\n",
        "        fixture_path,\n",
        "        X=X,\n",
        "        subject_id=subject_id,\n",
        "        rep_id=rep_id,\n",
        "        gesture_id=gesture_id,\n",
        "        exercise_id=exercise_id,\n",
        "        sample_start=sample_start,\n",
        "        sample_end=sample_end,\n",
        "    )\n",
        "\n",
        "    print(\"Wrote fixture:\", fixture_path)\n",
        "\n",
        "\n",
        "config_path = NOTEBOOK_DIR / \"colab_quickstart.yaml\"\n",
        "artifacts_root = str(OUT_ROOT / \"artifacts\")\n",
        "runs_root = str(OUT_ROOT / \"runs\")\n",
        "reports_root = str(OUT_ROOT / \"reports\")\n",
        "\n",
        "config_text = textwrap.dedent(\n",
        "    \"\"\"    profile: colab_quickstart\n",
        "\n",
        "    experiment:\n",
        "      id: colab_quickstart\n",
        "      artifacts_root: \"{artifacts_root}\"\n",
        "      runs_root: \"{runs_root}\"\n",
        "      reports_root: \"{reports_root}\"\n",
        "\n",
        "    dataset:\n",
        "      source: fixture\n",
        "      fixture_path: \"{fixture_path}\"\n",
        "      sampling_rate_hz: 2000\n",
        "      channels: 2\n",
        "      subjects: [1, 2]\n",
        "\n",
        "    preprocess:\n",
        "      id: fixture_tiny\n",
        "      window_ms: 4\n",
        "      hop_ms: 2\n",
        "      window_samples: 8\n",
        "      hop_samples: 4\n",
        "      cache: false\n",
        "      output:\n",
        "        windows_path: \"{{artifacts_root}}/data/{{preprocess_id}}/windows_s{{subject_id}}.npy\"\n",
        "        meta_path: \"{{artifacts_root}}/data/{{preprocess_id}}/meta.json\"\n",
        "\n",
        "    manifest:\n",
        "      id: fixture_tiny\n",
        "      output:\n",
        "        manifest_csv: \"{{artifacts_root}}/manifests/{{manifest_id}}/manifest.csv\"\n",
        "\n",
        "    splits:\n",
        "      cache: false\n",
        "      allow_missing_classes: true\n",
        "\n",
        "    protocols:\n",
        "      pooled_repdisjoint:\n",
        "        type: pooled_repdisjoint\n",
        "        output_dir: \"{artifacts_root}/splits/protocol=pooled_repdisjoint\"\n",
        "        reps:\n",
        "          all: [1, 2, 3]\n",
        "          test: [2]\n",
        "          val: [3]\n",
        "      loso:\n",
        "        type: loso\n",
        "        output_dir: \"{artifacts_root}/splits/protocol=loso\"\n",
        "        subjects: [1, 2]\n",
        "        reps:\n",
        "          all: [1, 2, 3]\n",
        "          test: [2]\n",
        "          val: [3]\n",
        "\n",
        "    models:\n",
        "      tiny_cnn:\n",
        "        architecture: ST_CNN_GN\n",
        "        params:\n",
        "          num_electrodes: 2\n",
        "          num_samples: 8\n",
        "          conv_channels: [4]\n",
        "          kernel_size: [3, 3]\n",
        "          pool_sizes: [[1, 1]]\n",
        "          conv_dropout: 0.0\n",
        "          gn_groups: 1\n",
        "          head_hidden: [8, 4]\n",
        "          head_dropout: 0.0\n",
        "          num_classes: 2\n",
        "\n",
        "    train:\n",
        "      device: cpu\n",
        "      seeds: [0]\n",
        "      max_steps: 10\n",
        "      max_epochs: 1\n",
        "      batch_size: 4\n",
        "      num_workers: 0\n",
        "      log_every: 1\n",
        "      optimizer:\n",
        "        name: adamw\n",
        "        lr: 0.001\n",
        "        weight_decay: 0.0\n",
        "      checkpoint:\n",
        "        primary: last\n",
        "        save_best: true\n",
        "        save_last: true\n",
        "      early_stopping:\n",
        "        enabled: false\n",
        "\n",
        "    eval:\n",
        "      max_batches: 5\n",
        "      latency:\n",
        "        enabled: false\n",
        "\n",
        "    plan:\n",
        "      models: [tiny_cnn]\n",
        "      protocols: [pooled_repdisjoint, loso]\n",
        "      max_jobs: 1\n",
        "    \"\"\"\n",
        ").format(\n",
        "    artifacts_root=artifacts_root,\n",
        "    runs_root=runs_root,\n",
        "    reports_root=reports_root,\n",
        "    fixture_path=fixture_path,\n",
        ")\n",
        "\n",
        "config_path.write_text(config_text, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Config written to:\", config_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "Config: `colab_quickstart.yaml`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tinyml_semg_classifier.cli import run_pipeline\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(config_path))\n",
        "run_pipeline(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "protocol_tables.md -> /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart/reports/protocol_tables.md\n",
            "#### Table 1 – Performance across evaluation protocols (mean ± std)\n",
            "\n",
            "| Protocol                     | Model       | Balanced Acc. [%] | Macro-F1 [%] |\n",
            "| ---------------------------- | ----------- | ----------------- | ------------ |\n",
            "| Single-subject, rep-disjoint | ST-CNN | n/a | n/a |\n",
            "| Pooled, rep-disjoint | ST-CNN | **45.0 ± 0.0** | 43.7 ± 0.0 |\n",
            "| Cross-subject (LOSO) | ST-CNN | **40.0 ± 0.0** | 38.8 ± 1.3 |\n",
            "\n",
            "### Table 2 – Protocol-Dependent Model Ranking\n",
            "\n",
            "| Evaluation Protocol                 | Primary Generalization Axis | Better Model | Δ Balanced Accuracy (pp) | Ranking Stability |\n",
            "| ----------------------------------- | --------------------------- | ------------ | ------------------------ | ----------------- |\n",
            "| Single-subject, repetition-disjoint | Repetitions (within-user) | n/a | n/a | n/a |\n",
            "| Pooled, repetition-disjoint | Repetitions (seen users) | ST-CNN | n/a | Stable |\n",
            "| Cross-subject (LOSO) | Subjects (unseen users) | ST-CNN | n/a | Stable |\n",
            "\n",
            "#### Table 3 – Protocol sensitivity (Balanced Accuracy)\n",
            "\n",
            "| Model       | Pooled BA [%] | LOSO BA [%] | Relative Drop [%] |\n",
            "| ----------- | ------------- | ----------- | ----------------- |\n",
            "| ST-CNN | 45.0 | 40.0 | **1111.1** |\n",
            "\n",
            "#### Table 4 – Model efficiency summary\n",
            "\n",
            "| Model       | Parameters | MACs / Inference Window | FLOPs / Inference Window | Inference Latency [ms] |\n",
            "| ----------- | ---------- | ----------------------- | ------------------------ | ---------------------- |\n",
            "| ST-CNN | 650 | 1.13k | 2.26k | n/a |\n",
            "| ST-Attn-CNN | n/a | n/a | n/a | n/a |\n",
            "\n",
            "metrics.json files: 3\n",
            "Example run -> /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart/runs/tiny_cnn/loso/test_subject=1/seed0/metrics.json\n",
            "{\n",
            "  \"model\": \"tiny_cnn\",\n",
            "  \"model_stats\": {\n",
            "    \"flops_per_window\": 2256,\n",
            "    \"latency\": null,\n",
            "    \"macs_per_window\": 1128,\n",
            "    \"num_parameters\": 650\n",
            "  },\n",
            "  \"protocol\": \"loso\",\n",
            "  \"protocol_instance\": {\n",
            "    \"test_subject\": 1\n",
            "  },\n",
            "  \"seed\": 0,\n",
            "  \"test\": {\n",
            "    \"accuracy\": 0.4,\n",
            "    \"balanced_accuracy\": 0.4000000059604645,\n",
            "    \"confusion_matrix\": {\n",
            "      \"labels\": [\n",
            "        \"1\",\n",
            "        \"2\"\n",
            "      ],\n",
            "      \"matrix\": [\n",
            "        [\n",
            "          1,\n",
            "          4\n",
            "        ],\n",
            "        [\n",
            "          2,\n",
            "          3\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"loss\": 0.7007931709289551,\n",
            "    \"macro_f1\": 0.375,\n",
            "    \"per_class\": {\n",
            "      \"accuracy\": {\n",
            "        \"1\": 0.20000000298023224,\n",
            "        \"2\": 0.6000000238418579\n",
            "      },\n",
            "      \"f1\": {\n",
            "        \"1\": 0.25,\n",
            "        \"2\": 0.5\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"train\": {\n",
            "    \"accuracy\": 0.2,\n",
            "    \"balanced_accuracy\": 0.20000000298023224,\n",
            "    \"loss\": 0.7982610464096069,\n",
            "    \"macro_f1\": 0.1666666716337204\n",
            "  },\n",
            "  \"training\": {\n",
            "    \"best_epoch\": 1,\n",
            "    \"checkpoint\": \"best\",\n",
            "    \"num_epochs\": 1,\n",
            "    \"primary_checkpoint\": \"last\"\n",
            "  },\n",
            "  \"val\": {\n",
            "    \"accuracy\": 0.4,\n",
            "    \"balanced_accuracy\": 0.4000000059604645,\n",
            "    \"loss\": 0.7145546078681946,\n",
            "    \"macro_f1\": 0.375\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sizing: `size`\n",
        "\n",
        "`size` benchmarks a few steps, probes concurrency, and estimates wall-time plus resources.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sizing.json -> /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart/artifacts/sizing/sizing.json\n",
            "{\n",
            "  \"baseline_per_model\": {\n",
            "    \"tiny_cnn\": {\n",
            "      \"rss_peak_gb\": 0.3553924560546875,\n",
            "      \"samples_sec\": 2895.87384846481,\n",
            "      \"vram_peak_gb\": 0.0\n",
            "    }\n",
            "  },\n",
            "  \"hardware_detected\": {\n",
            "    \"cpu_cores\": 8,\n",
            "    \"disk_free_gb\": 25.99346923828125,\n",
            "    \"gpu_name\": null,\n",
            "    \"ram_total_gb\": 16.0,\n",
            "    \"vram_total_gb\": null\n",
            "  },\n",
            "  \"probe_jobs_per_gpu\": [],\n",
            "  \"recommendation\": {\n",
            "    \"cpu_cores\": 1,\n",
            "    \"jobs_per_gpu\": 1,\n",
            "    \"ram_total_gb\": 0.5330886840820312,\n",
            "    \"ssd_gb\": 0.13186692167073488,\n",
            "    \"vram_per_gpu_gb\": 0.0\n",
            "  },\n",
            "  \"walltime_by_gpus\": [\n",
            "    {\n",
            "      \"concurrency\": 1,\n",
            "      \"gpus\": 1,\n",
            "      \"wall_hours\": 6.677479298827502e-06\n",
            "    }\n",
            "  ],\n",
            "  \"workload\": {\n",
            "    \"runs\": {\n",
            "      \"models\": [\n",
            "        \"tiny_cnn\"\n",
            "      ],\n",
            "      \"num_models\": 1,\n",
            "      \"num_seeds\": 1,\n",
            "      \"num_splits\": 3,\n",
            "      \"total_runs\": 3\n",
            "    },\n",
            "    \"splits\": {\n",
            "      \"count\": 3,\n",
            "      \"num_test_windows\": 40,\n",
            "      \"num_train_windows\": 40,\n",
            "      \"num_val_windows\": 40\n",
            "    },\n",
            "    \"splits_root\": \"/Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart/artifacts/splits\",\n",
            "    \"steps\": {\n",
            "      \"test\": 11,\n",
            "      \"total\": 33,\n",
            "      \"train\": 11,\n",
            "      \"val\": 11\n",
            "    },\n",
            "    \"train\": {\n",
            "      \"batch_size\": 4,\n",
            "      \"max_epochs\": 1,\n",
            "      \"max_steps\": 10,\n",
            "      \"num_seeds\": 1,\n",
            "      \"overrides\": {\n",
            "        \"max_epochs\": false,\n",
            "        \"num_seeds\": false\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "Recommendation: {'cpu_cores': 1, 'jobs_per_gpu': 1, 'ram_total_gb': 0.5330886840820312, 'ssd_gb': 0.13186692167073488, 'vram_per_gpu_gb': 0.0}\n",
            "Walltime by GPUs: [{'concurrency': 1, 'gpus': 1, 'wall_hours': 6.677479298827502e-06}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "from tinyml_semg_classifier.cli import size\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(config_path))\n",
        "\n",
        "size(\n",
        "    cfg,\n",
        "    warmup_steps=1,\n",
        "    bench_train_steps=5,\n",
        "    bench_val_steps=5,\n",
        "    device=\"cpu\",\n",
        "    max_k=1,\n",
        "    max_gpus=1,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "sizing_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"sizing.json\"\n",
        "sizing = json.loads(sizing_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"sizing.json ->\", sizing_path)\n",
        "print(json.dumps(sizing, indent=2)[:2000])\n",
        "\n",
        "recommendation = sizing.get(\"recommendation\") or {}\n",
        "if recommendation:\n",
        "    print(\"Recommendation:\", recommendation)\n",
        "\n",
        "walltime_by_gpus = sizing.get(\"walltime_by_gpus\") or []\n",
        "if walltime_by_gpus:\n",
        "    print(\"Walltime by GPUs:\", walltime_by_gpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Edit `colab_quickstart.yaml` to change models, protocols, or seeds.\n",
        "- Switch `profile` to `dev_mini` for a larger fixture run.\n",
        "- Point `dataset.source` to NinaPro data for a full run.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semg-protocol-sensitivity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
