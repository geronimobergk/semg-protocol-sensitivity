{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfcdf144",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5-10 min)\n",
        "\n",
        "This notebook is an executable entry point to the repository.\n",
        "\n",
        "It installs the package from Git and downloads the configs necessary to run a tiny **fixture** dataset (no NinaPro download):\n",
        "`prepare -> splits -> traineval -> report`, then runs `size` for quick sizing estimates.\n",
        "\n",
        "Outputs are written to `runs/colab_quickstart/` so your working tree stays clean.\n",
        "\n",
        "> This is a tutorial run (`--profile colab_quickstart`), not a benchmark for reporting results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebdb7ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ac708e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sh(cmd):\n",
        "    print(\"+\", \" \".join(map(str, cmd)))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "GIT_URL = \"git+https://github.com/geronimobergk/semg-protocol-sensitivity.git\"\n",
        "LOCAL_SRC = Path(\"..\").resolve()  # notebooks/ -> repo root\n",
        "\n",
        "# fast-path: already importable\n",
        "try:\n",
        "    import tinyml_semg_classifier  # noqa F401\n",
        "\n",
        "    print(\"Package already available â€” skipping install\")\n",
        "except Exception:\n",
        "    if shutil.which(\"uv\"):\n",
        "        if IN_COLAB:\n",
        "            sh([\"uv\", \"pip\", \"install\", GIT_URL])\n",
        "        else:\n",
        "            sh([\"uv\", \"pip\", \"install\", \"-e\", str(LOCAL_SRC)])\n",
        "    else:\n",
        "        if IN_COLAB:\n",
        "            sh([sys.executable, \"-m\", \"pip\", \"install\", GIT_URL])\n",
        "        else:\n",
        "            sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(LOCAL_SRC)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd9a555",
      "metadata": {},
      "outputs": [],
      "source": [
        "REL_CFG = \"configs/experiments/protocol_sensitivity_semg_cnn.yml\"\n",
        "REL_PROFILE = \"configs/profiles/colab_quickstart.yaml\"\n",
        "BASE_URL = \"https://raw.githubusercontent.com/geronimobergk/semg-protocol-sensitivity/fix/colab-notebook/\"\n",
        "\n",
        "cfg = Path(\"..\") / REL_CFG\n",
        "profile = Path(\"..\") / REL_PROFILE\n",
        "\n",
        "if cfg.exists() and profile.exists():\n",
        "    BASE_CFG, PROFILE = cfg.resolve(), profile.resolve()\n",
        "else:\n",
        "    tmp = Path(tempfile.mkdtemp())\n",
        "    BASE_CFG = tmp / Path(REL_CFG).name\n",
        "    PROFILE = tmp / Path(REL_PROFILE).name\n",
        "    subprocess.check_call([\"curl\", \"-L\", \"-o\", str(BASE_CFG), BASE_URL + REL_CFG])\n",
        "    subprocess.check_call([\"curl\", \"-L\", \"-o\", str(PROFILE), BASE_URL + REL_PROFILE])\n",
        "\n",
        "print(\"Using config:\", BASE_CFG)\n",
        "print(\"Using profile:\", PROFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc554bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2839a9b",
      "metadata": {},
      "source": [
        "## Configure outputs (via overrides)\n",
        "\n",
        "The base experiment config is loaded from the installed package. For a tutorial run, we redirect **all** outputs under `runs/colab_quickstart/`.\n",
        "\n",
        "We also generate a tiny fixture file locally so the profile does not depend on repo files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6d1009",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tinyml_semg_classifier.utils.io import read_yaml\n",
        "\n",
        "\n",
        "OUT_ROOT = (Path.cwd() / \"runs\" / \"colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fixture_path = OUT_ROOT / \"fixture_tiny.npz\"\n",
        "\n",
        "\n",
        "def fixture_needs_regen(path: Path) -> bool:\n",
        "    if not path.exists():\n",
        "        return True\n",
        "    try:\n",
        "        with np.load(path) as data:\n",
        "            required = [\n",
        "                \"X\",\n",
        "                \"subject_id\",\n",
        "                \"rep_id\",\n",
        "                \"gesture_id\",\n",
        "                \"exercise_id\",\n",
        "                \"sample_start\",\n",
        "                \"sample_end\",\n",
        "            ]\n",
        "            if any(name not in data for name in required):\n",
        "                return True\n",
        "            arrays = [np.asarray(data[name]) for name in required[1:]]\n",
        "            num_rows = arrays[0].shape[0]\n",
        "            if any(arr.shape[0] != num_rows for arr in arrays):\n",
        "                return True\n",
        "            rows = set(zip(*(arr.tolist() for arr in arrays)))\n",
        "            return len(rows) != num_rows\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "if fixture_needs_regen(fixture_path):\n",
        "    rng = np.random.default_rng(0)\n",
        "    subjects = [1, 2]\n",
        "    reps = [1, 2, 3]\n",
        "    gestures = [1, 2]\n",
        "    windows_per_combo = 5\n",
        "    records = [\n",
        "        (subject, rep, gesture)\n",
        "        for subject in subjects\n",
        "        for rep in reps\n",
        "        for gesture in gestures\n",
        "        for _ in range(windows_per_combo)\n",
        "    ]\n",
        "    num_windows = len(records)\n",
        "    num_electrodes = 2\n",
        "    num_samples = 8\n",
        "\n",
        "    X = rng.standard_normal((num_windows, num_electrodes, num_samples)).astype(\n",
        "        \"float32\"\n",
        "    )\n",
        "    subject_id = np.array([r[0] for r in records], dtype=\"int32\")\n",
        "    rep_id = np.array([r[1] for r in records], dtype=\"int32\")\n",
        "    gesture_id = np.array([r[2] for r in records], dtype=\"int32\")\n",
        "    exercise_id = np.ones(num_windows, dtype=\"int32\")\n",
        "    sample_start = np.arange(num_windows, dtype=\"int32\") * num_samples\n",
        "    sample_end = sample_start + (num_samples - 1)\n",
        "\n",
        "    np.savez(\n",
        "        fixture_path,\n",
        "        X=X,\n",
        "        subject_id=subject_id,\n",
        "        rep_id=rep_id,\n",
        "        gesture_id=gesture_id,\n",
        "        exercise_id=exercise_id,\n",
        "        sample_start=sample_start,\n",
        "        sample_end=sample_end,\n",
        "    )\n",
        "\n",
        "    print(\"Wrote fixture:\", fixture_path)\n",
        "\n",
        "\n",
        "overrides_path = OUT_ROOT / \"overrides_colab_quickstart.yaml\"\n",
        "overrides_path.write_text(\n",
        "    \"\"\"experiment:\n",
        "  artifacts_root: \"{artifacts_root}\"\n",
        "  runs_root: \"{runs_root}\"\n",
        "  reports_root: \"{reports_root}\"\n",
        "dataset:\n",
        "  fixture_path: \"{fixture_path}\"\n",
        "\"\"\".format(\n",
        "        artifacts_root=OUT_ROOT / \"artifacts\",\n",
        "        runs_root=OUT_ROOT / \"runs\",\n",
        "        reports_root=OUT_ROOT / \"reports\",\n",
        "        fixture_path=fixture_path,\n",
        "    ),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "overrides = read_yaml(overrides_path) or {}\n",
        "\n",
        "print(\"Overrides written to:\", overrides_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09d13a1",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "Profile: `colab_quickstart`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2c3546",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tinyml_semg_classifier.cli import run_pipeline\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(BASE_CFG), profile=PROFILE, overrides=[overrides])\n",
        "run_pipeline(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304e5412",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fd7a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb97de0c",
      "metadata": {},
      "source": [
        "## Sizing: `size`\n",
        "\n",
        "`size` benchmarks a few steps, probes concurrency, and estimates wall-time plus resources.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff01058",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from tinyml_semg_classifier.cli import size\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(BASE_CFG), profile=PROFILE, overrides=[overrides])\n",
        "\n",
        "size(\n",
        "    cfg,\n",
        "    warmup_steps=1,\n",
        "    bench_train_steps=5,\n",
        "    bench_val_steps=5,\n",
        "    device=\"cpu\",\n",
        "    max_k=1,\n",
        "    max_gpus=1,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "sizing_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"sizing.json\"\n",
        "sizing = json.loads(sizing_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"sizing.json ->\", sizing_path)\n",
        "print(json.dumps(sizing, indent=2)[:2000])\n",
        "\n",
        "recommendation = sizing.get(\"recommendation\") or {}\n",
        "if recommendation:\n",
        "    print(\"Recommendation:\", recommendation)\n",
        "\n",
        "walltime_by_gpus = sizing.get(\"walltime_by_gpus\") or []\n",
        "if walltime_by_gpus:\n",
        "    print(\"Walltime by GPUs:\", walltime_by_gpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2634e321",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Switch `PROFILE` to `smoke` or `dev_mini` for a larger fixture run.\n",
        "- Run without a profile (or with `dry_run`) for real NinaPro data.\n",
        "- Use `configs/experiments/protocol_sensitivity_semg_cnn.yml` to change protocols, models, or seeds.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semg-protocol-sensitivity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
