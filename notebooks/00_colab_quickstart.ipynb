{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfcdf144",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5-10 min)\n",
        "\n",
        "This notebook is an executable entry point to the repository.\n",
        "\n",
        "It installs the package from Git and downloads the configs necessary to run a tiny **fixture** dataset (no NinaPro download):\n",
        "`prepare -> splits -> traineval -> report`, then runs `size` for quick sizing estimates.\n",
        "\n",
        "Outputs are written to `runs/colab_quickstart/` so your working tree stays clean.\n",
        "\n",
        "> This is a tutorial run (`--profile colab_quickstart`), not a benchmark for reporting results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7ebdb7ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43ac708e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package already available — skipping install\n"
          ]
        }
      ],
      "source": [
        "def sh(cmd):\n",
        "    print(\"+\", \" \".join(map(str, cmd)))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "GIT_URL = \"git+https://github.com/geronimobergk/semg-protocol-sensitivity.git@fix/colab-notebook\"\n",
        "LOCAL_SRC = Path(\"..\").resolve()  # notebooks/ → repo root\n",
        "\n",
        "# fast-path: already importable\n",
        "try:\n",
        "    import tinyml_semg_classifier  # noqa F401\n",
        "\n",
        "    print(\"Package already available — skipping install\")\n",
        "except Exception:\n",
        "    if shutil.which(\"uv\"):\n",
        "        target = GIT_URL if IN_COLAB else str(LOCAL_SRC)\n",
        "        sh([\"uv\", \"pip\", \"install\", target])\n",
        "    else:\n",
        "        # last-resort fallback (pip must exist)\n",
        "        sh(\n",
        "            [\n",
        "                sys.executable,\n",
        "                \"-m\",\n",
        "                \"pip\",\n",
        "                \"install\",\n",
        "                GIT_URL if IN_COLAB else \"-e\",\n",
        "                str(LOCAL_SRC),\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9bd9a555",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using config: /Users/geronimo/Projects/semg-protocol-sensitivity/configs/experiments/protocol_sensitivity_semg_cnn.yml\n",
            "Using profile: /Users/geronimo/Projects/semg-protocol-sensitivity/configs/profiles/colab_quickstart.yaml\n"
          ]
        }
      ],
      "source": [
        "REL_CFG = \"configs/experiments/protocol_sensitivity_semg_cnn.yml\"\n",
        "REL_PROFILE = \"configs/profiles/colab_quickstart.yaml\"\n",
        "BASE_URL = \"https://raw.githubusercontent.com/geronimobergk/semg-protocol-sensitivity/fix/colab-notebook/\"\n",
        "\n",
        "cfg = Path(\"..\") / REL_CFG\n",
        "profile = Path(\"..\") / REL_PROFILE\n",
        "\n",
        "if cfg.exists() and profile.exists():\n",
        "    BASE_CFG, PROFILE = cfg.resolve(), profile.resolve()\n",
        "else:\n",
        "    tmp = Path(tempfile.mkdtemp())\n",
        "    BASE_CFG = tmp / Path(REL_CFG).name\n",
        "    PROFILE = tmp / Path(REL_PROFILE).name\n",
        "    subprocess.check_call([\"curl\", \"-L\", \"-o\", str(BASE_CFG), BASE_URL + REL_CFG])\n",
        "    subprocess.check_call([\"curl\", \"-L\", \"-o\", str(PROFILE), BASE_URL + REL_PROFILE])\n",
        "\n",
        "print(\"Using config:\", BASE_CFG)\n",
        "print(\"Using profile:\", PROFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cbc554bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: 3.14.0 | torch: 2.9.1 | cuda: False\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2839a9b",
      "metadata": {},
      "source": [
        "## Configure outputs (via overrides)\n",
        "\n",
        "The base experiment config is loaded from the installed package. For a tutorial run, we redirect **all** outputs under `runs/colab_quickstart/`.\n",
        "\n",
        "We also generate a tiny fixture file locally so the profile does not depend on repo files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4a6d1009",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overrides written to: /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart/overrides_colab_quickstart.yaml\n",
            "Outputs root: /Users/geronimo/Projects/semg-protocol-sensitivity/notebooks/runs/colab_quickstart\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tinyml_semg_classifier.utils.io import read_yaml\n",
        "\n",
        "\n",
        "OUT_ROOT = (Path.cwd() / \"runs\" / \"colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fixture_path = OUT_ROOT / \"fixture_tiny.npz\"\n",
        "if not fixture_path.exists():\n",
        "    rng = np.random.default_rng(0)\n",
        "    subjects = [1, 2]\n",
        "    reps = [1, 2, 3]\n",
        "    gestures = [1, 2]\n",
        "    windows_per_combo = 5\n",
        "    records = [\n",
        "        (subject, rep, gesture)\n",
        "        for subject in subjects\n",
        "        for rep in reps\n",
        "        for gesture in gestures\n",
        "        for _ in range(windows_per_combo)\n",
        "    ]\n",
        "    num_windows = len(records)\n",
        "    num_electrodes = 2\n",
        "    num_samples = 8\n",
        "\n",
        "    X = rng.standard_normal((num_windows, num_electrodes, num_samples)).astype(\n",
        "        \"float32\"\n",
        "    )\n",
        "    subject_id = np.array([r[0] for r in records], dtype=\"int32\")\n",
        "    rep_id = np.array([r[1] for r in records], dtype=\"int32\")\n",
        "    gesture_id = np.array([r[2] for r in records], dtype=\"int32\")\n",
        "    exercise_id = np.ones(num_windows, dtype=\"int32\")\n",
        "    sample_start = np.zeros(num_windows, dtype=\"int32\")\n",
        "    sample_end = np.full(num_windows, num_samples - 1, dtype=\"int32\")\n",
        "\n",
        "    np.savez(\n",
        "        fixture_path,\n",
        "        X=X,\n",
        "        subject_id=subject_id,\n",
        "        rep_id=rep_id,\n",
        "        gesture_id=gesture_id,\n",
        "        exercise_id=exercise_id,\n",
        "        sample_start=sample_start,\n",
        "        sample_end=sample_end,\n",
        "    )\n",
        "\n",
        "    print(\"Wrote fixture:\", fixture_path)\n",
        "\n",
        "\n",
        "overrides_path = OUT_ROOT / \"overrides_colab_quickstart.yaml\"\n",
        "overrides_path.write_text(\n",
        "    \"\"\"experiment:\n",
        "  artifacts_root: \"{artifacts_root}\"\n",
        "  runs_root: \"{runs_root}\"\n",
        "  reports_root: \"{reports_root}\"\n",
        "dataset:\n",
        "  fixture_path: \"{fixture_path}\"\n",
        "\"\"\".format(\n",
        "        artifacts_root=OUT_ROOT / \"artifacts\",\n",
        "        runs_root=OUT_ROOT / \"runs\",\n",
        "        reports_root=OUT_ROOT / \"reports\",\n",
        "        fixture_path=fixture_path,\n",
        "    ),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "overrides = read_yaml(overrides_path) or {}\n",
        "\n",
        "print(\"Overrides written to:\", overrides_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09d13a1",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "Profile: `colab_quickstart`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea2c3546",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Manifest contains duplicate sample_id values.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtinyml_semg_classifier\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config\n\u001b[32m      5\u001b[39m cfg = load_config(\u001b[38;5;28mstr\u001b[39m(BASE_CFG), profile=PROFILE, overrides=[overrides])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semg-protocol-sensitivity/tinyml_semg_classifier/cli.py:78\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(cfg, max_jobs)\u001b[39m\n\u001b[32m     76\u001b[39m download(cfg)\n\u001b[32m     77\u001b[39m prepare(cfg)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m traineval(cfg, max_jobs=max_jobs)\n\u001b[32m     80\u001b[39m report(cfg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semg-protocol-sensitivity/tinyml_semg_classifier/cli.py:32\u001b[39m, in \u001b[36msplits\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplits\u001b[39m(cfg: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43msplits_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semg-protocol-sensitivity/tinyml_semg_classifier/datasets/splits/__init__.py:17\u001b[39m, in \u001b[36mgenerate_splits\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_splits\u001b[39m(cfg: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Path]:\n\u001b[32m     16\u001b[39m     manifest_path = Path(cfg[\u001b[33m\"\u001b[39m\u001b[33mmanifest\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmanifest_csv\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     manifest_rows = \u001b[43mload_manifest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanifest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     manifest_hash = hash_file(manifest_path)\n\u001b[32m     19\u001b[39m     splits_cfg = cfg.get(\u001b[33m\"\u001b[39m\u001b[33msplits\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semg-protocol-sensitivity/tinyml_semg_classifier/datasets/manifest.py:94\u001b[39m, in \u001b[36mload_manifest\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     92\u001b[39m     reader = csv.DictReader(handle)\n\u001b[32m     93\u001b[39m     rows = [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mvalidate_manifest_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rows\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semg-protocol-sensitivity/tinyml_semg_classifier/datasets/manifest.py:75\u001b[39m, in \u001b[36mvalidate_manifest_rows\u001b[39m\u001b[34m(rows)\u001b[39m\n\u001b[32m     73\u001b[39m     sample_ids.append(\u001b[38;5;28mstr\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33msample_id\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample_ids) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(sample_ids)):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mManifest contains duplicate sample_id values.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: Manifest contains duplicate sample_id values."
          ]
        }
      ],
      "source": [
        "from tinyml_semg_classifier.cli import run_pipeline\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(BASE_CFG), profile=PROFILE, overrides=[overrides])\n",
        "run_pipeline(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304e5412",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fd7a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb97de0c",
      "metadata": {},
      "source": [
        "## Sizing: `size`\n",
        "\n",
        "`size` benchmarks a few steps, probes concurrency, and estimates wall-time plus resources.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff01058",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from tinyml_semg_classifier.cli import size\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(BASE_CFG), profile=PROFILE, overrides=[overrides])\n",
        "\n",
        "size(\n",
        "    cfg,\n",
        "    warmup_steps=1,\n",
        "    bench_train_steps=5,\n",
        "    bench_val_steps=5,\n",
        "    device=\"cpu\",\n",
        "    max_k=1,\n",
        "    max_gpus=1,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "sizing_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"sizing.json\"\n",
        "sizing = json.loads(sizing_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"sizing.json ->\", sizing_path)\n",
        "print(json.dumps(sizing, indent=2)[:2000])\n",
        "\n",
        "recommendation = sizing.get(\"recommendation\") or {}\n",
        "if recommendation:\n",
        "    print(\"Recommendation:\", recommendation)\n",
        "\n",
        "walltime_by_gpus = sizing.get(\"walltime_by_gpus\") or []\n",
        "if walltime_by_gpus:\n",
        "    print(\"Walltime by GPUs:\", walltime_by_gpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2634e321",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Switch `PROFILE` to `smoke` or `dev_mini` for a larger fixture run.\n",
        "- Run without a profile (or with `dry_run`) for real NinaPro data.\n",
        "- Use `configs/experiments/protocol_sensitivity_semg_cnn.yml` to change protocols, models, or seeds.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semg-protocol-sensitivity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
