{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5–10 min)\n",
        "\n",
        "This notebook is an executable entry point to the repository.\n",
        "\n",
        "It runs the full end-to-end pipeline on a tiny **fixture** dataset (no NinaPro download):\n",
        "`prepare → splits → traineval → report`, then runs `bench` + `estimate`.\n",
        "\n",
        "Outputs are written to `runs/colab_quickstart/` so your working tree stays clean.\n",
        "\n",
        "> This is a smoke test / tutorial run (`--profile smoke`), not a benchmark for reporting results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def sh(cmd):\n",
        "    print(\"+\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "GIT_REPO = \"git+https://github.com/geronimobergk/semg-protocol-sensitivity.git\"\n",
        "LOCAL_SRC = \"..\"\n",
        "\n",
        "target = GIT_REPO if IN_COLAB else LOCAL_SRC\n",
        "\n",
        "sh([sys.executable, \"-m\", \"pip\", \"install\", target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc554bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure outputs (via `--overrides`)\n",
        "\n",
        "The base experiment config writes to `artifacts/`, `runs/`, and `reports/`. For a tutorial run, we redirect **all** outputs under `runs/colab_quickstart/`.\n",
        "\n",
        "This keeps committed report files untouched while still showcasing the full pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "OUT_ROOT = Path(\"runs/colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "overrides_path = OUT_ROOT / \"overrides_colab_quickstart.yaml\"\n",
        "overrides_path.write_text(\n",
        "    \"\"\"experiment:\n",
        "  artifacts_root: \\\"{artifacts_root}\\\"\n",
        "  runs_root: \\\"{runs_root}\\\"\n",
        "  reports_root: \\\"{reports_root}\\\"\\n\n",
        "\"\"\".format(\n",
        "        artifacts_root=OUT_ROOT / \"artifacts\",\n",
        "        runs_root=OUT_ROOT / \"runs\",\n",
        "        reports_root=OUT_ROOT / \"reports\",\n",
        "    ),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Overrides written to:\", overrides_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "All with `--profile smoke`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "BASE_CONFIG = \"configs/experiments/protocol_sensitivity_semg_cnn.yml\"\n",
        "PROFILE = \"smoke\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"run\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sizing: `bench` + `estimate`\n",
        "\n",
        "`bench` measures step time (and GPU memory if on CUDA). `estimate` uses split counts + the bench results to compute end-to-end runtime and resource estimates.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd_bench = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"bench\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "    \"--warmup-steps\",\n",
        "    \"1\",\n",
        "    \"--measure-steps\",\n",
        "    \"5\",\n",
        "    \"--device\",\n",
        "    \"cpu\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd_bench))\n",
        "subprocess.run(cmd_bench, check=True)\n",
        "\n",
        "cmd_estimate = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"estimate\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "    \"--gpus\",\n",
        "    \"1\",\n",
        "    \"--alpha\",\n",
        "    \"1.0\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd_estimate))\n",
        "subprocess.run(cmd_estimate, check=True)\n",
        "\n",
        "bench_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"bench.json\"\n",
        "estimate_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"estimate.json\"\n",
        "\n",
        "bench = json.loads(bench_path.read_text(encoding=\"utf-8\"))\n",
        "estimate = json.loads(estimate_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"bench.json ->\", bench_path)\n",
        "print(json.dumps(bench, indent=2)[:2000])\n",
        "\n",
        "print(\"estimate.json ->\", estimate_path)\n",
        "print(\"Wall-time hours (estimate):\", estimate[\"time_hours\"][\"wall\"])\n",
        "print(\"Peak VRAM GB (estimate):\", estimate[\"resources\"][\"gpu_vram_gb\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Switch `PROFILE` to `dev_mini` for a slightly larger fixture run.\n",
        "- Run without a profile (or with `dry_run`) for real NinaPro data.\n",
        "- Use `configs/experiments/protocol_sensitivity_semg_cnn.yml` to change protocols, models, or seeds."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ninapro-db1-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
