{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfcdf144",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5-10 min)\n",
        "\n",
        "This notebook is an executable entry point to the repository.\n",
        "\n",
        "It runs the full end-to-end pipeline on a tiny **fixture** dataset (no NinaPro download):\n",
        "`prepare -> splits -> traineval -> report`, then runs `size` for quick sizing estimates.\n",
        "\n",
        "Outputs are written to `runs/colab_quickstart/` so your working tree stays clean.\n",
        "\n",
        "> This is a tutorial run (`--profile colab_quickstart`), not a benchmark for reporting results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ac708e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def sh(cmd, cwd=None):\n",
        "    print(\"+\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd, cwd=cwd)\n",
        "\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "REPO_URL = \"https://github.com/geronimobergk/semg-protocol-sensitivity.git\"\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for candidate in [start] + list(start.parents):\n",
        "        if (candidate / \"pyproject.toml\").exists() and (candidate / \"configs\").exists():\n",
        "            return candidate\n",
        "    raise FileNotFoundError(f\"Repo root not found from: {start}\")\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    REPO_ROOT = Path(\"/content/semg-protocol-sensitivity\")\n",
        "    if not REPO_ROOT.exists():\n",
        "        sh([\"git\", \"clone\", REPO_URL, str(REPO_ROOT)])\n",
        "else:\n",
        "    REPO_ROOT = find_repo_root(Path.cwd())\n",
        "\n",
        "sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"], cwd=REPO_ROOT)\n",
        "\n",
        "os.chdir(REPO_ROOT)\n",
        "print(\"Repo root:\", REPO_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc554bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2839a9b",
      "metadata": {},
      "source": [
        "## Configure outputs (via `--overrides`)\n",
        "\n",
        "The base experiment config writes to `artifacts/`, `runs/`, and `reports/`. For a tutorial run, we redirect **all** outputs under `runs/colab_quickstart/`.\n",
        "\n",
        "This keeps committed report files untouched while still showcasing the full pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6d1009",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "OUT_ROOT = (REPO_ROOT / \"runs\" / \"colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "overrides_path = OUT_ROOT / \"overrides_colab_quickstart.yaml\"\n",
        "overrides_path.write_text(\n",
        "    \"\"\"experiment:\n",
        "  artifacts_root: \"{artifacts_root}\"\n",
        "  runs_root: \"{runs_root}\"\n",
        "  reports_root: \"{reports_root}\"\n",
        "\"\"\".format(\n",
        "        artifacts_root=OUT_ROOT / \"artifacts\",\n",
        "        runs_root=OUT_ROOT / \"runs\",\n",
        "        reports_root=OUT_ROOT / \"reports\",\n",
        "    ),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Overrides written to:\", overrides_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09d13a1",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "All with `--profile colab_quickstart`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2c3546",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "BASE_CONFIG = str(REPO_ROOT / \"configs/experiments/protocol_sensitivity_semg_cnn.yml\")\n",
        "PROFILE = \"colab_quickstart\"\n",
        "CLI = [sys.executable, \"-m\", \"tinyml_semg_classifier.cli\"]\n",
        "\n",
        "cmd = CLI + [\n",
        "    \"run\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304e5412",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fd7a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb97de0c",
      "metadata": {},
      "source": [
        "## Sizing: `size`\n",
        "\n",
        "`size` benchmarks a few steps, probes concurrency, and estimates wall-time plus resources.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff01058",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd_size = CLI + [\n",
        "    \"size\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "    \"--warmup-steps\",\n",
        "    \"1\",\n",
        "    \"--bench-train-steps\",\n",
        "    \"5\",\n",
        "    \"--bench-val-steps\",\n",
        "    \"5\",\n",
        "    \"--device\",\n",
        "    \"cpu\",\n",
        "    \"--max-k\",\n",
        "    \"1\",\n",
        "    \"--max-gpus\",\n",
        "    \"1\",\n",
        "    \"--alpha\",\n",
        "    \"1.0\",\n",
        "]\n",
        "print(\"Running:\", \" \").join(cmd_size)\n",
        "subprocess.run(cmd_size, check=True)\n",
        "\n",
        "sizing_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"sizing.json\"\n",
        "sizing = json.loads(sizing_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"sizing.json ->\", sizing_path)\n",
        "print(json.dumps(sizing, indent=2)[:2000])\n",
        "\n",
        "recommendation = sizing.get(\"recommendation\") or {}\n",
        "if recommendation:\n",
        "    print(\"Recommendation:\", recommendation)\n",
        "\n",
        "walltime_by_gpus = sizing.get(\"walltime_by_gpus\") or []\n",
        "if walltime_by_gpus:\n",
        "    print(\"Walltime by GPUs:\", walltime_by_gpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2634e321",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Switch `PROFILE` to `smoke` or `dev_mini` for a larger fixture run.\n",
        "- Run without a profile (or with `dry_run`) for real NinaPro data.\n",
        "- Use `configs/experiments/protocol_sensitivity_semg_cnn.yml` to change protocols, models, or seeds.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ninapro-db1-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
