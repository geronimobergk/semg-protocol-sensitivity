{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5-10 min)\n",
        "\n",
        "This notebook installs the package from Git and writes a minimal experiment config next to the notebook.\n",
        "\n",
        "It runs the full end-to-end pipeline on a tiny fixture dataset (no NinaPro download):\n",
        "`prepare -> splits -> traineval -> report`, then runs `size` for quick sizing estimates.\n",
        "\n",
        "Outputs are written under `runs/colab_quickstart/`.\n",
        "\n",
        "> This is a tutorial run (smoke-like config), not a benchmark for reporting results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be37ccd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sh(cmd):\n",
        "    print(\"+\", \" \".join(map(str, cmd)))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "GIT_URL = \"git+https://github.com/geronimobergk/semg-protocol-sensitivity.git\"\n",
        "\n",
        "try:\n",
        "    import tinyml_semg_classifier  # noqa F401\n",
        "\n",
        "    print(\"Package already available - skipping install\")\n",
        "except Exception:\n",
        "    repo_root = Path.cwd().resolve()\n",
        "    if (repo_root / \"pyproject.toml\").exists():\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_root)])\n",
        "    elif (repo_root.parent / \"pyproject.toml\").exists():\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_root.parent)])\n",
        "    else:\n",
        "        sh([sys.executable, \"-m\", \"pip\", \"install\", GIT_URL])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check\n",
        "import torch\n",
        "\n",
        "print(\n",
        "    \"python:\",\n",
        "    sys.version.split()[0],\n",
        "    \"| torch:\",\n",
        "    torch.__version__,\n",
        "    \"| cuda:\",\n",
        "    torch.cuda.is_available(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a tiny experiment config\n",
        "\n",
        "We generate a small fixture and write a minimal smoke-like config next to the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd().resolve()\n",
        "OUT_ROOT = (NOTEBOOK_DIR / \"runs\" / \"colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fixture_path = OUT_ROOT / \"fixture_tiny.npz\"\n",
        "\n",
        "\n",
        "def fixture_needs_regen(path: Path) -> bool:\n",
        "    if not path.exists():\n",
        "        return True\n",
        "    try:\n",
        "        with np.load(path) as data:\n",
        "            required = [\n",
        "                \"X\",\n",
        "                \"subject_id\",\n",
        "                \"rep_id\",\n",
        "                \"gesture_id\",\n",
        "                \"exercise_id\",\n",
        "                \"sample_start\",\n",
        "                \"sample_end\",\n",
        "            ]\n",
        "            if any(name not in data for name in required):\n",
        "                return True\n",
        "            arrays = [np.asarray(data[name]) for name in required[1:]]\n",
        "            num_rows = arrays[0].shape[0]\n",
        "            if any(arr.shape[0] != num_rows for arr in arrays):\n",
        "                return True\n",
        "            rows = set(zip(*(arr.tolist() for arr in arrays)))\n",
        "            return len(rows) != num_rows\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "if fixture_needs_regen(fixture_path):\n",
        "    rng = np.random.default_rng(0)\n",
        "    subjects = [1, 2]\n",
        "    reps = [1, 2, 3]\n",
        "    gestures = [1, 2]\n",
        "    windows_per_combo = 5\n",
        "    records = [\n",
        "        (subject, rep, gesture)\n",
        "        for subject in subjects\n",
        "        for rep in reps\n",
        "        for gesture in gestures\n",
        "        for _ in range(windows_per_combo)\n",
        "    ]\n",
        "    num_windows = len(records)\n",
        "    num_electrodes = 2\n",
        "    num_samples = 8\n",
        "\n",
        "    X = rng.standard_normal((num_windows, num_electrodes, num_samples)).astype(\n",
        "        \"float32\"\n",
        "    )\n",
        "    subject_id = np.array([r[0] for r in records], dtype=\"int32\")\n",
        "    rep_id = np.array([r[1] for r in records], dtype=\"int32\")\n",
        "    gesture_id = np.array([r[2] for r in records], dtype=\"int32\")\n",
        "    exercise_id = np.ones(num_windows, dtype=\"int32\")\n",
        "    sample_start = np.arange(num_windows, dtype=\"int32\") * num_samples\n",
        "    sample_end = sample_start + (num_samples - 1)\n",
        "\n",
        "    np.savez(\n",
        "        fixture_path,\n",
        "        X=X,\n",
        "        subject_id=subject_id,\n",
        "        rep_id=rep_id,\n",
        "        gesture_id=gesture_id,\n",
        "        exercise_id=exercise_id,\n",
        "        sample_start=sample_start,\n",
        "        sample_end=sample_end,\n",
        "    )\n",
        "\n",
        "    print(\"Wrote fixture:\", fixture_path)\n",
        "\n",
        "\n",
        "config_path = NOTEBOOK_DIR / \"colab_quickstart.yaml\"\n",
        "artifacts_root = str(OUT_ROOT / \"artifacts\")\n",
        "runs_root = str(OUT_ROOT / \"runs\")\n",
        "reports_root = str(OUT_ROOT / \"reports\")\n",
        "\n",
        "config_text = textwrap.dedent(\n",
        "    \"\"\"    profile: colab_quickstart\n",
        "\n",
        "    experiment:\n",
        "      id: colab_quickstart\n",
        "      artifacts_root: \"{artifacts_root}\"\n",
        "      runs_root: \"{runs_root}\"\n",
        "      reports_root: \"{reports_root}\"\n",
        "\n",
        "    dataset:\n",
        "      source: fixture\n",
        "      fixture_path: \"{fixture_path}\"\n",
        "      sampling_rate_hz: 2000\n",
        "      channels: 2\n",
        "      subjects: [1, 2]\n",
        "\n",
        "    preprocess:\n",
        "      id: fixture_tiny\n",
        "      window_ms: 4\n",
        "      hop_ms: 2\n",
        "      window_samples: 8\n",
        "      hop_samples: 4\n",
        "      cache: false\n",
        "      output:\n",
        "        windows_path: \"{{artifacts_root}}/data/{{preprocess_id}}/windows_s{{subject_id}}.npy\"\n",
        "        meta_path: \"{{artifacts_root}}/data/{{preprocess_id}}/meta.json\"\n",
        "\n",
        "    manifest:\n",
        "      id: fixture_tiny\n",
        "      output:\n",
        "        manifest_csv: \"{{artifacts_root}}/manifests/{{manifest_id}}/manifest.csv\"\n",
        "\n",
        "    splits:\n",
        "      cache: false\n",
        "      allow_missing_classes: true\n",
        "\n",
        "    protocols:\n",
        "      pooled_repdisjoint:\n",
        "        type: pooled_repdisjoint\n",
        "        output_dir: \"{artifacts_root}/splits/protocol=pooled_repdisjoint\"\n",
        "        reps:\n",
        "          all: [1, 2, 3]\n",
        "          test: [2]\n",
        "          val: [3]\n",
        "      loso:\n",
        "        type: loso\n",
        "        output_dir: \"{artifacts_root}/splits/protocol=loso\"\n",
        "        subjects: [1, 2]\n",
        "        reps:\n",
        "          all: [1, 2, 3]\n",
        "          test: [2]\n",
        "          val: [3]\n",
        "\n",
        "    models:\n",
        "      tiny_cnn:\n",
        "        architecture: ST_CNN_GN\n",
        "        params:\n",
        "          num_electrodes: 2\n",
        "          num_samples: 8\n",
        "          conv_channels: [4]\n",
        "          kernel_size: [3, 3]\n",
        "          pool_sizes: [[1, 1]]\n",
        "          conv_dropout: 0.0\n",
        "          gn_groups: 1\n",
        "          head_hidden: [8, 4]\n",
        "          head_dropout: 0.0\n",
        "          num_classes: 2\n",
        "\n",
        "    train:\n",
        "      device: cpu\n",
        "      seeds: [0]\n",
        "      max_steps: 10\n",
        "      max_epochs: 1\n",
        "      batch_size: 4\n",
        "      num_workers: 0\n",
        "      log_every: 1\n",
        "      optimizer:\n",
        "        name: adamw\n",
        "        lr: 0.001\n",
        "        weight_decay: 0.0\n",
        "      checkpoint:\n",
        "        primary: last\n",
        "        save_best: true\n",
        "        save_last: true\n",
        "      early_stopping:\n",
        "        enabled: false\n",
        "\n",
        "    eval:\n",
        "      max_batches: 5\n",
        "      latency:\n",
        "        enabled: false\n",
        "\n",
        "    plan:\n",
        "      models: [tiny_cnn]\n",
        "      protocols: [pooled_repdisjoint, loso]\n",
        "      max_jobs: 1\n",
        "    \"\"\"\n",
        ").format(\n",
        "    artifacts_root=artifacts_root,\n",
        "    runs_root=runs_root,\n",
        "    reports_root=reports_root,\n",
        "    fixture_path=fixture_path,\n",
        ")\n",
        "\n",
        "config_path.write_text(config_text, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Config written to:\", config_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "Config: `colab_quickstart.yaml`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tinyml_semg_classifier.cli import run_pipeline\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(config_path))\n",
        "run_pipeline(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sizing: `size`\n",
        "\n",
        "`size` benchmarks a few steps, probes concurrency, and estimates wall-time plus resources.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from tinyml_semg_classifier.cli import size\n",
        "from tinyml_semg_classifier.config import load_config\n",
        "\n",
        "\n",
        "cfg = load_config(str(config_path))\n",
        "\n",
        "size(\n",
        "    cfg,\n",
        "    warmup_steps=1,\n",
        "    bench_train_steps=5,\n",
        "    bench_val_steps=5,\n",
        "    device=\"cpu\",\n",
        "    max_k=1,\n",
        "    max_gpus=1,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "sizing_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"sizing.json\"\n",
        "sizing = json.loads(sizing_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"sizing.json ->\", sizing_path)\n",
        "print(json.dumps(sizing, indent=2)[:2000])\n",
        "\n",
        "recommendation = sizing.get(\"recommendation\") or {}\n",
        "if recommendation:\n",
        "    print(\"Recommendation:\", recommendation)\n",
        "\n",
        "walltime_by_gpus = sizing.get(\"walltime_by_gpus\") or []\n",
        "if walltime_by_gpus:\n",
        "    print(\"Walltime by GPUs:\", walltime_by_gpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Edit `colab_quickstart.yaml` to change models, protocols, or seeds.\n",
        "- Switch `profile` to `dev_mini` for a larger fixture run.\n",
        "- Point `dataset.source` to NinaPro data for a full run.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semg-protocol-sensitivity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
