{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Quickstart (5–10 min)\n",
        "\n",
        "This notebook is an executable entry point to the repository.\n",
        "\n",
        "It runs the full end-to-end pipeline on a tiny **fixture** dataset (no NinaPro download):\n",
        "`prepare → splits → traineval → report`, then runs `bench` + `estimate`.\n",
        "\n",
        "Outputs are written to `runs/colab_quickstart/` so your working tree stays clean.\n",
        "\n",
        "> This is a smoke test / tutorial run (`--profile smoke`), not a benchmark for reporting results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/geronimos/tinyml-semig-classifier.git\"\n",
        "REPO_DIR = Path(\"tinyml-semig-classifier\")\n",
        "\n",
        "\n",
        "def _run(cmd: list[str]) -> None:\n",
        "    print(\"+\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def _is_repo_root(path: Path) -> bool:\n",
        "    return (\n",
        "        (path / \"pyproject.toml\").exists()\n",
        "        and (path / \"tinyml_semg_classifier\").is_dir()\n",
        "        and (path / \"configs/experiments/protocol_sensitivity_semg_cnn.yml\").exists()\n",
        "    )\n",
        "\n",
        "\n",
        "def _find_repo_root(start: Path) -> Path | None:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if _is_repo_root(candidate):\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "repo_root = _find_repo_root(Path.cwd())\n",
        "if repo_root is None:\n",
        "    if not REPO_DIR.exists():\n",
        "        _run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\n",
        "    repo_root = REPO_DIR.resolve()\n",
        "\n",
        "os.chdir(repo_root)\n",
        "commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], text=True).strip()\n",
        "print(\"Repo:\", Path.cwd())\n",
        "print(\"Commit:\", commit)\n",
        "\n",
        "# Install the project into the *current* Python environment (kernel).\n",
        "# We prefer `uv` because many uv-managed environments do not include `pip`.\n",
        "uv = shutil.which(\"uv\")\n",
        "if uv is None:\n",
        "    try:\n",
        "        _run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"uv\"])\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(\n",
        "            \"Missing `uv` and could not install it. Install uv from https://docs.astral.sh/uv/ \"\n",
        "            \"or run this notebook in an environment with pip.\"\n",
        "        ) from exc\n",
        "    uv = shutil.which(\"uv\")\n",
        "if uv is None:\n",
        "    raise RuntimeError(\"`uv` is required but was not found on PATH.\")\n",
        "\n",
        "_run([uv, \"pip\", \"install\", \"--python\", sys.executable, \"-e\", \".\"])\n",
        "\n",
        "torch = importlib.import_module(\"torch\")\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure outputs (via `--overrides`)\n",
        "\n",
        "The base experiment config writes to `artifacts/`, `runs/`, and `reports/`. For a tutorial run, we redirect **all** outputs under `runs/colab_quickstart/`.\n",
        "\n",
        "This keeps committed report files untouched while still showcasing the full pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "OUT_ROOT = Path(\"runs/colab_quickstart\").resolve()\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "overrides_path = OUT_ROOT / \"overrides_colab_quickstart.yaml\"\n",
        "overrides_path.write_text(\n",
        "    \"\"\"experiment:\n",
        "  artifacts_root: \\\"{artifacts_root}\\\"\n",
        "  runs_root: \\\"{runs_root}\\\"\n",
        "  reports_root: \\\"{reports_root}\\\"\\n\n",
        "\"\"\".format(\n",
        "        artifacts_root=OUT_ROOT / \"artifacts\",\n",
        "        runs_root=OUT_ROOT / \"runs\",\n",
        "        reports_root=OUT_ROOT / \"reports\",\n",
        "    ),\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Overrides written to:\", overrides_path)\n",
        "print(\"Outputs root:\", OUT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the tiny end-to-end pipeline\n",
        "\n",
        "This executes:\n",
        "\n",
        "- `prepare` (fixture preprocessing)\n",
        "- `splits` (pooled rep-disjoint + LOSO)\n",
        "- `traineval` (tiny CNN, capped steps)\n",
        "- `report` (aggregated tables)\n",
        "\n",
        "All with `--profile smoke`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "BASE_CONFIG = \"configs/experiments/protocol_sensitivity_semg_cnn.yml\"\n",
        "PROFILE = \"smoke\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"run\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect outputs\n",
        "\n",
        "We print the generated protocol tables and one example `metrics.json` to confirm the pipeline produced results end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "reports_root = OUT_ROOT / \"reports\"\n",
        "tables_path = reports_root / \"protocol_tables.md\"\n",
        "\n",
        "print(\"protocol_tables.md ->\", tables_path)\n",
        "print(tables_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "metrics_paths = sorted((OUT_ROOT / \"runs\").rglob(\"metrics.json\"))\n",
        "print(\"metrics.json files:\", len(metrics_paths))\n",
        "if metrics_paths:\n",
        "    sample = metrics_paths[0]\n",
        "    print(\"Example run ->\", sample)\n",
        "    payload = json.loads(sample.read_text(encoding=\"utf-8\"))\n",
        "    print(json.dumps(payload, indent=2)[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sizing: `bench` + `estimate`\n",
        "\n",
        "`bench` measures step time (and GPU memory if on CUDA). `estimate` uses split counts + the bench results to compute end-to-end runtime and resource estimates.\n",
        "\n",
        "We keep this tiny and CPU-only for Colab speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd_bench = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"bench\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "    \"--warmup-steps\",\n",
        "    \"1\",\n",
        "    \"--measure-steps\",\n",
        "    \"5\",\n",
        "    \"--device\",\n",
        "    \"cpu\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd_bench))\n",
        "subprocess.run(cmd_bench, check=True)\n",
        "\n",
        "cmd_estimate = [\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"tinyml_semg_classifier.cli\",\n",
        "    \"estimate\",\n",
        "    \"-c\",\n",
        "    BASE_CONFIG,\n",
        "    \"--profile\",\n",
        "    PROFILE,\n",
        "    \"--overrides\",\n",
        "    str(overrides_path),\n",
        "    \"--gpus\",\n",
        "    \"1\",\n",
        "    \"--alpha\",\n",
        "    \"1.0\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd_estimate))\n",
        "subprocess.run(cmd_estimate, check=True)\n",
        "\n",
        "bench_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"bench.json\"\n",
        "estimate_path = OUT_ROOT / \"artifacts\" / \"sizing\" / \"estimate.json\"\n",
        "\n",
        "bench = json.loads(bench_path.read_text(encoding=\"utf-8\"))\n",
        "estimate = json.loads(estimate_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"bench.json ->\", bench_path)\n",
        "print(json.dumps(bench, indent=2)[:2000])\n",
        "\n",
        "print(\"estimate.json ->\", estimate_path)\n",
        "print(\"Wall-time hours (estimate):\", estimate[\"time_hours\"][\"wall\"])\n",
        "print(\"Peak VRAM GB (estimate):\", estimate[\"resources\"][\"gpu_vram_gb\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Switch `PROFILE` to `dev_mini` for a slightly larger fixture run.\n",
        "- Run without a profile (or with `dry_run`) for real NinaPro data.\n",
        "- Use `configs/experiments/protocol_sensitivity_semg_cnn.yml` to change protocols, models, or seeds."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_colab_quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tinyml-semg-classifier",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
